{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sentencepiece in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (6.32.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172e750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc62a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59904ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_HIDDEN_LAYERS = 16\n",
    "MAX_LENGTH = 16\n",
    "EVAL_INTERVAL = 10\n",
    "LEARNING_RATE = 9e-4\n",
    "EPOCHS = 6\n",
    "N_EMBD = 128\n",
    "N_HEAD = 8\n",
    "N_LAYER = 8\n",
    "DROPOUT = 0.4\n",
    "IMG_SIZE = 96\n",
    "PATCH_SIZE = 16\n",
    "IMAGE_EMBED_DIM = 512\n",
    "N_CHANNELS = 3\n",
    "MAX_POSITION_EMBEDDINGS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c32d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc56a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>caption</th>\n",
       "      <th>b64string_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car.png</td>\n",
       "      <td>red car</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAABuwAAAPoCAYAAAA1H9jcAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astronaut.png</td>\n",
       "      <td>astronaut in a white space suit</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAABLAAAASwCAMAAADc/0P9AA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv.png</td>\n",
       "      <td>black television on a table</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAABH4AAANRCAYAAACV6Ht0AA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horse.png</td>\n",
       "      <td>brown horse running</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAADawAAAjGCAYAAAC3JWiEAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine.png</td>\n",
       "      <td>wine bottle</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAYAAAD0eNT6AA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file                          caption  \\\n",
       "0        car.png                          red car   \n",
       "1  astronaut.png  astronaut in a white space suit   \n",
       "2         tv.png      black television on a table   \n",
       "3      horse.png              brown horse running   \n",
       "4       wine.png                      wine bottle   \n",
       "\n",
       "                                    b64string_images  \n",
       "0  iVBORw0KGgoAAAANSUhEUgAABuwAAAPoCAYAAAA1H9jcAA...  \n",
       "1  iVBORw0KGgoAAAANSUhEUgAABLAAAASwCAMAAADc/0P9AA...  \n",
       "2  iVBORw0KGgoAAAANSUhEUgAABH4AAANRCAYAAACV6Ht0AA...  \n",
       "3  iVBORw0KGgoAAAANSUhEUgAADawAAAjGCAYAAAC3JWiEAA...  \n",
       "4  iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAYAAAD0eNT6AA...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = '/mnt/d/Dysk/REPO/fine_tune_llm/simple-image-captions/'  # Directory containing images\n",
    "\n",
    "def image_file_to_base64(image_filename):\n",
    "    image_path = image_dir + image_filename\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        b64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    return b64_str\n",
    "\n",
    "df = pd.read_csv(image_dir + 'inputs.csv', sep=\";\").dropna(axis=1, how=\"all\")\n",
    "df['b64string_images'] = df['file'].apply(image_file_to_base64)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2043e0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ViTConfig(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_channels=N_CHANNELS,\n",
    "    hidden_size=IMAGE_EMBED_DIM,\n",
    "    num_attention_heads=N_HEAD,\n",
    "    num_hidden_layers=N_HIDDEN_LAYERS,\n",
    "    intermediate_size=4 * IMAGE_EMBED_DIM,\n",
    "    hidden_dropout_prob=DROPOUT,\n",
    "    attention_probs_dropout_prob=DROPOUT,\n",
    ")\n",
    "\n",
    "testvit = ViTModel(config)\n",
    "vit_input = torch.zeros(BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "testvit_out = testvit(vit_input).last_hidden_state[:, 0] # Get the [CLS] token representation\n",
    "testvit_out.shape # (BATCH_SIZE, IMAGE_EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d0e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 32000])\n"
     ]
    }
   ],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed,\n",
    "        image_embed_dim,\n",
    "        vocab_size,\n",
    "        n_layer,\n",
    "        n_head,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_hidden_layers,\n",
    "        dropout,\n",
    "        pad_token_id,\n",
    "        max_position_embeddings,\n",
    "        n_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_channels=n_channels,\n",
    "            hidden_size=image_embed_dim,\n",
    "            num_attention_heads=n_head,\n",
    "            num_hidden_layers=n_hidden_layers,\n",
    "            intermediate_size=4 * image_embed_dim,\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "        )\n",
    "        self.vision_encoder = ViTModel(vit_config)\n",
    "        self.image_projector = nn.Linear(image_embed_dim, n_embed)\n",
    "\n",
    "        llama_config = LlamaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=n_embed,\n",
    "            num_hidden_layers=n_layer,\n",
    "            num_attention_heads=n_head,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            pad_token_id=int(pad_token_id),\n",
    "        )\n",
    "        self.llama = LlamaForCausalLM(llama_config)\n",
    "        self.llama = self.llama.to(dtype=torch.bfloat16)  # Move Llama to bfloat16\n",
    "\n",
    "    def forward(self, img_array, input_ids, targets=None):\n",
    "        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n",
    "        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n",
    "        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]  # [BATCH_SIZE, IMAGE_EMBED_DIM]\n",
    "        image_embeds_proj = self.image_projector(image_embeds).to(dtype=torch.bfloat16)  # [BATCH_SIZE, N_EMBED]\n",
    "        image_embeds_proj = image_embeds_proj.unsqueeze(1) # [BATCH_SIZE, 1, N_EMBED]\n",
    "\n",
    "        text_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)  # [BATCH_SIZE, MAX_LENGTH, N_EMBED]\n",
    "\n",
    "        input_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)  # [BATCH_SIZE, MAX_LENGTH + 1, N_EMBED]\n",
    "\n",
    "        attention_mask = torch.ones(input_embeds.shape[:2], dtype=torch.long, device=input_embeds.device) # [BATCH_SIZE, MAX_LENGTH + 1]\n",
    "\n",
    "        if targets is not None:\n",
    "            #target: [BATCH_SIZE, MAX_LENGTH]\n",
    "            targets = torch.cat([torch.full((targets.size(0), 1), -100, dtype=targets.dtype, device=targets.device), targets], dim=1) # [BATCH_SIZE, MAX_LENGTH + 1]\n",
    "            outputs = self.llama(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets,\n",
    "            )\n",
    "            return outputs.logits, outputs.loss\n",
    "        else:\n",
    "            outputs = self.llama(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            return outputs.logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, img_array, input_ids, max_new_tokens=20):\n",
    "        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n",
    "        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n",
    "        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]\n",
    "        image_embeds_proj = self.image_projector(image_embeds).unsqueeze(1).to(dtype=torch.bfloat16)\n",
    "\n",
    "        input_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)\n",
    "        inputs_embeds = torch.cat([image_embeds_proj, input_embeds], dim=1)\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=inputs_embeds.device)\n",
    "       \n",
    "        generated = self.llama.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.llama.config.pad_token_id,\n",
    "            eos_token_id=self.llama.config.eos_token_id,\n",
    "        )\n",
    "        return generated\n",
    "\n",
    "model = VisionLanguageModel(\n",
    "        N_EMBD,\n",
    "        IMAGE_EMBED_DIM,\n",
    "        tokenizer.vocab_size,\n",
    "        N_LAYER,\n",
    "        N_HEAD,\n",
    "        IMG_SIZE,\n",
    "        PATCH_SIZE,\n",
    "        N_HIDDEN_LAYERS,\n",
    "        DROPOUT,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        n_channels=N_CHANNELS,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "dummy_img = torch.randn(1, N_CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\n",
    "dummy_idx = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH)).to(device)\n",
    "output = model(dummy_img, dummy_idx)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcab7cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated description of the given picture:\n",
      "uMil коро Datasons gleThencope hab perform rgba PackcmBysdl familia inicial Gregcowo parish sociétégetimaining\n"
     ]
    }
   ],
   "source": [
    "img_path = '/mnt/d/Dysk/REPO/fine_tune_llm/simple-image-captions/ship.png'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0).to(device)  # Shape: [BATCH_SIZE (1), N_CHANNELS, IMG_SIZE, IMG_SIZE]\n",
    "\n",
    "prompt = \"A photo of\" \n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(img_tensor, input_ids, max_new_tokens=30)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated description of the given picture:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5835144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64_to_tensor(base64_str, img_size=96):\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(base64_str)))\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690732d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class VLMDataset(Dataset):\n",
    "    def __init__(self, df, img_size=96, tokenizer=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_b64 = self.df.loc[idx, 'b64string_images']\n",
    "        caption = self.df.loc[idx, 'caption']\n",
    "        image = base64_to_tensor(img_b64, self.img_size).squeeze(0)\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze(0)\n",
    "        targets = input_ids.clone()\n",
    "        targets[:-1] = input_ids[1:]\n",
    "        targets[-1] = self.tokenizer.pad_token_id\n",
    "        return image, input_ids, targets\n",
    "\n",
    "df = pd.concat([df] * 50)[['b64string_images', 'caption']]\n",
    "n = int(0.9 * len(df))\n",
    "df_train = df.iloc[:n]\n",
    "df_val = df.iloc[n:]\n",
    "\n",
    "train_dataset = VLMDataset(df_train, img_size=IMG_SIZE, tokenizer=tokenizer)\n",
    "val_dataset = VLMDataset(df_val, img_size=IMG_SIZE, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(train_dataset[0]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebde21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_loader):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for images, input_ids, targets in val_loader:\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "        _, loss = model(images, input_ids, targets)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826837f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, eval_interval):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (images, input_ids, targets) in loop:\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(images, input_ids, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % eval_interval == 0:\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "        val_loss = estimate_loss(model, val_loader)\n",
    "        print(f\"Validation Loss after epoch {epoch}: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c9e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:   0%|          | 0/45 [00:00<?, ?it/s]/mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1/6: 100%|██████████| 45/45 [00:34<00:00,  1.31it/s, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 0: 2.316671133041382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|██████████| 45/45 [00:33<00:00,  1.35it/s, loss=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 1: 1.13691908121109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|██████████| 45/45 [00:32<00:00,  1.39it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 2: 0.4876415431499481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|██████████| 45/45 [00:33<00:00,  1.33it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 3: 0.23735306411981583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|██████████| 45/45 [00:33<00:00,  1.35it/s, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 4: 0.25926100462675095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|██████████| 45/45 [00:33<00:00,  1.32it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 5: 0.3212343454360962\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, EVAL_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a3fb303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated description of the given picture:\n",
      "in glass.\n"
     ]
    }
   ],
   "source": [
    "img_path = '/mnt/d/Dysk/REPO/fine_tune_llm/simple-image-captions/umbrella.png'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "prompt = \"A photo of\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(img_tensor, input_ids, max_new_tokens=30)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated description of the given picture:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb962a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
