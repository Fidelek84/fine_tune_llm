{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8541c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'simple-image-captions' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/uygarkurt/simple-image-captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a8599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sentencepiece in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /mnt/d/Dysk/REPO/fine_tune_llm/.venv/lib/python3.12/site-packages (6.32.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "172e750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fc62a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59904ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_HIDDEN_LAYERS = 16\n",
    "MAX_LENGTH = 16\n",
    "EVAL_INTERVAL = 10\n",
    "LEARNING_RATE = 9e-4\n",
    "EPOCHS = 6\n",
    "N_EMBD = 128\n",
    "N_HEAD = 8\n",
    "N_LAYER = 8\n",
    "DROPOUT = 0.4\n",
    "IMG_SIZE = 96\n",
    "PATCH_SIZE = 16\n",
    "IMAGE_EMBED_DIM = 512\n",
    "N_CHANNELS = 3\n",
    "MAX_POSITION_EMBEDDINGS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c32d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc56a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>caption</th>\n",
       "      <th>b64string_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car.png</td>\n",
       "      <td>red car</td>\n",
       "      <td>dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astronaut.png</td>\n",
       "      <td>astronaut in a white space suit</td>\n",
       "      <td>dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv.png</td>\n",
       "      <td>black television on a table</td>\n",
       "      <td>dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horse.png</td>\n",
       "      <td>brown horse running</td>\n",
       "      <td>dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine.png</td>\n",
       "      <td>wine bottle</td>\n",
       "      <td>dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file                          caption  \\\n",
       "0        car.png                          red car   \n",
       "1  astronaut.png  astronaut in a white space suit   \n",
       "2         tv.png      black television on a table   \n",
       "3      horse.png              brown horse running   \n",
       "4       wine.png                      wine bottle   \n",
       "\n",
       "                                    b64string_images  \n",
       "0  dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...  \n",
       "1  dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...  \n",
       "2  dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...  \n",
       "3  dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...  \n",
       "4  dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = '/mnt/d/Dysk/REPO/fine_tune_llm/simple-image-captions/'  # Directory containing images\n",
    "\n",
    "def image_file_to_base64(image_filename):\n",
    "    image_path = image_dir + image_filename\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        b64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    return b64_str\n",
    "\n",
    "df = pd.read_csv(image_dir + 'inputs.csv', sep=\";\").dropna(axis=1, how=\"all\")\n",
    "df['b64string_images'] = df['file'].apply(image_file_to_base64)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2043e0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ViTConfig(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_channels=N_CHANNELS,\n",
    "    hidden_size=IMAGE_EMBED_DIM,\n",
    "    num_attention_heads=N_HEAD,\n",
    "    num_hidden_layers=N_HIDDEN_LAYERS,\n",
    "    intermediate_size=4 * IMAGE_EMBED_DIM,\n",
    "    hidden_dropout_prob=DROPOUT,\n",
    "    attention_probs_dropout_prob=DROPOUT,\n",
    ")\n",
    "\n",
    "testvit = ViTModel(config)\n",
    "vit_input = torch.zeros(BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "testvit_out = testvit(vit_input).last_hidden_state[:, 0] # Get the [CLS] token representation\n",
    "testvit_out.shape # (BATCH_SIZE, IMAGE_EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82d0e19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m model.to(device)\n\u001b[32m    108\u001b[39m dummy_img = torch.randn(\u001b[32m1\u001b[39m, N_CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m dummy_idx = torch.randint(\u001b[32m0\u001b[39m, tokenizer.vocab_size, (\u001b[32m1\u001b[39m, \u001b[43mMAX_LENGTH\u001b[49m)).to(device)\n\u001b[32m    110\u001b[39m output = model(dummy_img, dummy_idx)\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(output.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed,\n",
    "        image_embed_dim,\n",
    "        vocab_size,\n",
    "        n_layer,\n",
    "        n_head,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_hidden_layers,\n",
    "        dropout,\n",
    "        pad_token_id,\n",
    "        max_position_embeddings,\n",
    "        n_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_channels=n_channels,\n",
    "            hidden_size=image_embed_dim,\n",
    "            num_attention_heads=n_head,\n",
    "            num_hidden_layers=n_hidden_layers,\n",
    "            intermediate_size=4 * image_embed_dim,\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "        )\n",
    "        self.vision_encoder = ViTModel(vit_config)\n",
    "        self.image_projector = nn.Linear(image_embed_dim, n_embed)\n",
    "\n",
    "        llama_config = LlamaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=n_embed,\n",
    "            num_hidden_layers=n_layer,\n",
    "            num_attention_heads=n_head,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            pad_token_id=int(pad_token_id),\n",
    "        )\n",
    "        self.llama = LlamaForCausalLM(llama_config)\n",
    "        self.llama = self.llama.to(dtype=torch.bfloat16)  # Move Llama to bfloat16\n",
    "\n",
    "    def forward(self, img_array, input_ids, targets=None):\n",
    "        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n",
    "        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n",
    "        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]  # [BATCH_SIZE, IMAGE_EMBED_DIM]\n",
    "        image_embeds_proj = self.image_projector(image_embeds).to(dtype=torch.bfloat16)  # [BATCH_SIZE, N_EMBED]\n",
    "        image_embeds_proj = image_embeds_proj.unsqueeze(1) # [BATCH_SIZE, 1, N_EMBED]\n",
    "\n",
    "        text_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)  # [BATCH_SIZE, MAX_LENGTH, N_EMBED]\n",
    "\n",
    "        input_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)  # [BATCH_SIZE, MAX_LENGTH + 1, N_EMBED]\n",
    "\n",
    "        attention_mask = torch.ones(input_embeds.shape[:2], dtype=torch.long, device=input_embeds.device) # [BATCH_SIZE, MAX_LENGTH + 1]\n",
    "\n",
    "        if targets is not None:\n",
    "            #target: [BATCH_SIZE, MAX_LENGTH]\n",
    "            targets = torch.cat([torch.full((targets.size(0), 1), -100, dtype=targets.dtype, device=targets.device), targets], dim=1) # [BATCH_SIZE, MAX_LENGTH + 1]\n",
    "            outputs = self.llama(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets,\n",
    "            )\n",
    "            return outputs.logits, outputs.loss\n",
    "        else:\n",
    "            outputs = self.llama(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            return outputs.logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, img_array, input_ids, max_new_tokens=20):\n",
    "        # img_array: [BATCH_SIZE, N_CHANNELS, IMG_SIZE, IMG_SIZE]\n",
    "        # input_ids: [BATCH_SIZE, MAX_LENGTH]\n",
    "        image_embeds = self.vision_encoder(img_array).last_hidden_state[:, 0]\n",
    "        image_embeds_proj = self.image_projector(image_embeds).unsqueeze(1).to(dtype=torch.bfloat16)\n",
    "\n",
    "        input_embeds = self.llama.model.embed_tokens(input_ids).to(dtype=torch.bfloat16)\n",
    "        inputs_embeds = torch.cat([image_embeds_proj, input_embeds], dim=1)\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=inputs_embeds.device)\n",
    "       \n",
    "        generated = self.llama.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.llama.config.pad_token_id,\n",
    "            eos_token_id=self.llama.config.eos_token_id,\n",
    "        )\n",
    "        return generated\n",
    "\n",
    "model = VisionLanguageModel(\n",
    "        N_EMBD,\n",
    "        IMAGE_EMBED_DIM,\n",
    "        tokenizer.vocab_size,\n",
    "        N_LAYER,\n",
    "        N_HEAD,\n",
    "        IMG_SIZE,\n",
    "        PATCH_SIZE,\n",
    "        N_HIDDEN_LAYERS,\n",
    "        DROPOUT,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        n_channels=N_CHANNELS,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "dummy_img = torch.randn(1, N_CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\n",
    "dummy_idx = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH)).to(device)\n",
    "output = model(dummy_img, dummy_idx)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab7cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
